# -*- coding: utf-8 -*-
"""Tensorflowproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10qnx3Q0DW5yVBpJrVog3T4cqj-ey8vRl
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

X_train = X_train/255

X_test = X_test/255

len(X_train)

len(X_test)

X_train[0].shape

X_train[0]

#0 means black, 255 means white

plt.matshow(X_train[2])

y_train[2]

y_train[:5]

X_train_flattened = X_train.reshape(len(X_train),28*28)
X_test_flattened = X_test.reshape(len(X_test), 28 * 28)

X_train_flattened.shape

X_test_flattened.shape

X_train_flattened[0]

model = keras.Sequential([
    keras.layers.Dense(10, input_shape = (784,),activation='sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs = 5)

model.evaluate(X_test_flattened, y_test)

plt.matshow(X_test[0])

y_predict = model.predict(X_test_flattened)
y_predict[0]

np.argmax(y_predict[4])

y_predicted_labels = [np.argmax(i) for i in y_predict]
y_predicted_labels[:5]

cm = tf.math.confusion_matrix(labels=y_test, predictions = y_predicted_labels)
cm

import seaborn as sn
plt.figure(figsize = (10,8))
sn.heatmap(cm, annot = True, fmt ='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

#this is for hidden layer
model = keras.Sequential([
    keras.layers.Dense(100, input_shape = (784,),activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])
#100 = no of neurons
model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs = 5)

model.evaluate(X_test_flattened, y_test)

#this is for hidden layer
#you can flatten layers instead of going the other route
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(100, input_shape = (784,),activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])
#100 = no of neurons
model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train, y_train, epochs = 5)

#this is for hidden layer
#you can flatten layers instead of going the other route
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(150, input_shape = (784,),activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])
#100 = no of neurons
model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train, y_train, epochs = 10)